
\documentclass[a4paper,12pt]{article}

\usepackage{indentfirst}
\usepackage[pdftex]{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[catalan]{babel}

\newcommand{\espai}{\par\vspace{5mm}}
\newcommand{\mylist}{
\begin{itemize}
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}}
\newcommand{\mylistend}{\end{itemize}}

\usepackage{titling}
\setlength{\droptitle}{-5cm}

\title{\bf Contextualization and Bibliography}
\author{
  {\bf Author}: Miquel Sabaté Solà
  \and
  {\bf Director}: Jordi García Almiñana
}
\date{March 6, 2014}

\begin{document}

% Title
\clearpage\maketitle

\setcounter{page}{1}
\section{Context}

This section aims to describe the context within which the project will be
developed. We differntiate between: developers, the tutor, library developers
and the users.

\subsection{Developers}

I am the only developer of this project. I will make use of my knowledge on the
topic, the guidance of my tutor and the previous work of the library developers
to accomplish my goals.

\subsection{Tutor}

Jordi García Almiñana is my tutor. His role is to make sure that I
have both my focus and my goals straight. He will be guiding me through the
development of this project and giving me some tips of advice.

\subsection{Library developers}

Library developers are all the developers that have contributed to the
libraries that I will be using to build this project. There are a lot of
developers, coming from different backgrounds and goals.

\subsection{Users}

The users are everyone that will make use of this project. We can distinguish
different users:

\mylist
\item End users that will make use of the data generated from our platform.
\item Developers that will make use of our services.
\item Developers that will read the code of this project, so they can get a
benefit from it.
\mylistend

\section{State of the art}

In this section I will be describing the state of the art. That is, I will
describe what is the current situation, the problems arising, and solutions
from other developers.

\subsection{Big data}

Big data\cite{wikibigdata} is a term that has been coined lately. Big data is
the term for a collection of data sets so large and complex that it becomes
difficult to process using on-hand database management tools or traditional
data processing applications.

Big data is difficult to work with using most relational database management
systems and desktop statistics and visualization packages, requiring instead
``massively parallel software running on tens, hundreds, or even thousands of
servers''. The definition of Big data varies depending on the organization.

\subsection{Map Reduce}

Since we cannot perform operations on Big data on a traditional way, we have to
build tools that will tackle this problem accordingly. The first step was made
by Google with its ``MapReduce'' algorithm\cite{mapreduce}. The MapReduce
algorithm solves this problem by mapping a specified problem (e.g. a file with
data inside of it) and finally reducing the results back. This is an easy way
to distribute the work on different machines, so they are as efficient as
possible.

Multiple implementations of MapReduce arised after the announcement of
MapReduce. The most famous implementation being Hadoop\cite{hadoop}. Hadoop
implements the algorithm of MapReduce with fault tolerance. It also provides
the idea of HDFS (Hadoop File System), a distributed file system.

\subsection{Streaming}

At this point, it is clear that Hadoop and the MapReduce algorithm brings a lot
of advantages and opportunities. This is nice but Hadoop does not do one thing:
realtime operations. Hadoop and similar technologies are not realtime systems,
nor are they meant to be. Hadoop is not designed to be a realtime system, and
thus any effort to hack Hadoop in any realtime direction is a waste of time.
This is because realtime data processing has a fundamentally different set of
requirements than batch processing.

This is really a problem, because realtime data processing at massive scale is
becoming more and more of a requirement these days for businesses. To fill this
hole, technologies such as Yahoo! S4 and Twitter Storm\cite{storm} were
developed. Right now Yahoo! S4 is more or less abandoned and Twitter Storm is
really alive, but under the umbrella of the Apache foundation\cite{apache} This
is why my project will be based on Storm.

Before Storm, you would typically have to manually build a network of queues and
workers to do realtime processing. Workers would process messages off a queue,
update databases, and send new messages to other queues for further processing.
Unfortunately, this approach has serious limitations:

\mylist
\item {\bf Tedious}: You spend most of your development time configuring where to send
messages, deploying workers, and deploying intermediate queues. The realtime
processing logic that you care about corresponds to a relatively small
percentage of your codebase.
\item {\bf Painful} to scale: When the message throughput get too high for a single worker
or queue, you need to partition how the data is spread around. You need to
reconfigure the other workers to know the new locations to send messages. This
introduces moving parts and new pieces that can fail.
\mylistend

Apart from Storm, I will make use of other technologies such as
Summingbird\cite{summingbird} and other technologies from Twitter\cite{twitter}
and Yahoo\cite{yahoo}. These two companies have open sourced a lot of libraries
for Storm that will be useful in the development of this project.

\subsection{Future}

The future of Big data will be tied in the future to the streaming
technologies. They solve increasingly important issues and they perform efficiently
when dealing with lots of data.

It's clear, then, that more companies will follow this lead. This implies more
support and more libraries available out there.

\subsection{Smart cities}

Finally, I would like to talk about the concept of Smart Cities\cite{smart}.
Smart cities is a new concept that embodies all the efforts to make cities more
efficient and more reliable.

That key component are IT technologies. We use data centers, sensors, etc. to
compute key elements. With all the computed data, we can then analyze which
parts of the cities can be improved in order to make citizens happier and
services more efficient.

\newpage

\renewcommand\bibname{Bibliography}
\begin{thebibliography}{9}

\bibitem{wikibigdata}
  Dan Kusnetzky,
  {\emph What is Big Data?}.
  \url{http://www.zdnet.com/blog/virtualization/what-is-big-data/1708}",
  2010.

\bibitem{mapreduce}
  Jeffrey Dean and Sanjay Ghemawat,
  {\emph MapReduce: Simplified Data Processing on Large Clusters}
  2004.

\bibitem{hadoop}
  Hadoop. Wikipedia.
  \url{http://en.wikipedia.org/wiki/Hadoop}

\bibitem{storm}
  Storm.
  \url{http://storm.incubator.apache.org/}

\bibitem{apache}
  Apache Software Foundation.
  \url{http://www.apache.org/}

\bibitem{summingbird}
  Github page for Summingbird.
  \url{https://github.com/twitter/summingbird}

\bibitem{smart}
  Smart Cities.
  \url{http://en.wikipedia.org/wiki/Smart_cities}

\bibitem{twitter}
  Github page of Twitter.
  \url{https://github.com/twitter}

\bibitem{yahoo}
  Github page of Yahoo!.
  \url{https://github.com/yahoo}

\end{thebibliography}

\end{document}
